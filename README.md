# LLM-Efficiency-Engine
Intelligent LLM gateway reducing AI costs through semantic caching, contrastive routing, and carbon-aware scheduling
